{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7f583b",
   "metadata": {},
   "source": [
    "# Predicting non-response to ketamine for depression: a symptom-level analysis of real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df768c9",
   "metadata": {},
   "source": [
    "## Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35702351",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Local helpers\n",
    "from lib.helpers import *\n",
    "\n",
    "# Python core\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "# Scipy family\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# Other\n",
    "import pingouin as pg\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.anova import AnovaRM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0bb6e",
   "metadata": {},
   "source": [
    "## Establish global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "panels_output_directory = './panels/'\n",
    "outdir = os.path.expanduser(panels_output_directory)\n",
    "\n",
    "preprocess_output_directory = './preprocess/'\n",
    "preprocess_outdir = os.path.expanduser(preprocess_output_directory)\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42  # embed fonts so we can edit in Illustrator\n",
    "resolution = 1000  # DPI\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd374b",
   "metadata": {},
   "source": [
    "## Load raw data and confirm valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c529bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "all_data = pd.read_csv('./data.csv')\n",
    "all_data['sex'] = [0 if s=='M' else 1 for s in all_data['sex'].values]\n",
    "unique_patient_ids = np.unique(all_data['ID'].values)\n",
    "\n",
    "# Grab the essential columns of data for subsequent analysis\n",
    "all_phq_item_cols = ['phqitem' + str(i) for i in np.arange(1, 10)]\n",
    "essential_cols_phq = ['ID', 'age', 'sex', 'route', 'sessionNumber', 'sessionDay', 'phqtotal'] + all_phq_item_cols\n",
    "d_phq = all_data[essential_cols_phq]\n",
    "d_phq = d_phq[~np.isnan(d_phq).any(axis=1)]\n",
    "d_phq = d_phq.sort_values(by='ID')\n",
    "\n",
    "# Ensure that patients have valid baseline and longitudinal data\n",
    "valid_phq_subjects = []\n",
    "for patient_idx, patient_id in enumerate(unique_patient_ids):\n",
    "    d_patient = d_phq[d_phq['ID']==patient_id]\n",
    "    if d_patient.shape[0] == 0:\n",
    "        print(\"PHQ-9: no data at all for patient %d\" % patient_id)\n",
    "    elif np.min(d_patient['sessionNumber']) != 1:\n",
    "        print(\"PHQ-9: no baseline data for patient %d\" % patient_id)\n",
    "    elif np.max(d_patient['sessionNumber']) < 2:\n",
    "        print(\"PHQ-9: less than 2 repeats for patient %d\" % patient_id)\n",
    "    else:\n",
    "        valid_phq_subjects.append(patient_id) \n",
    "d_phq = d_phq[d_phq['ID'].isin(valid_phq_subjects)].sort_values(by='ID')\n",
    "\n",
    "print(\"%d of %d subjects have valid data\" % (len(valid_phq_subjects), len(unique_patient_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12296887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# mdd = 0\n",
    "# bl = []\n",
    "# data = all_data[all_data['sessionNumber']==1]\n",
    "# for pid in unique_patient_ids:\n",
    "#     i += 1\n",
    "#     d = data[data['ID']==pid]\n",
    "#     diag = d['diagnosis'].values[0].lower()\n",
    "#     if 'major depression' in diag or 'mdd' in diag or 'major depressive' in diag or 'depression' in diag:\n",
    "#         mdd += 1\n",
    "#     else:\n",
    "#         print(diag)\n",
    "#         bl.append(d['phqtotal'].values[0])\n",
    "#         print('\\n')\n",
    "# print(mdd / i)\n",
    "# print(np.mean(bl))\n",
    "# print(bl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f48b5",
   "metadata": {},
   "source": [
    "## Basic patient demographics and absolute dose ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f911c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.DataFrame(columns=['ID', 'age', 'sex', 'route', 'dose_min', 'dose_max'])\n",
    "ix = 0\n",
    "for patient_id in unique_patient_ids:\n",
    "    d_patient = all_data[all_data['ID']==patient_id]\n",
    "    sex = d_patient['sex'].values[0]\n",
    "    route = np.unique(d_patient['route'].values)[0]\n",
    "    age = d_patient['age'].values[0]\n",
    "    dose_min = d_patient['dose'].min()\n",
    "    dose_max = d_patient['dose'].max()\n",
    "    demo.loc[ix] = [patient_id, age, sex, route, dose_min, dose_max]\n",
    "    ix += 1\n",
    "    \n",
    "# Demographics\n",
    "num_female = demo[demo['sex']==1].shape[0]\n",
    "num_male = demo[demo['sex']==0].shape[0]\n",
    "num_IN = demo[demo['route']==1].shape[0]\n",
    "num_IV = demo[demo['route']==2].shape[0]\n",
    "num_IM = demo[demo['route']==3].shape[0]\n",
    "ages = demo['age'].values\n",
    "# Doses min\n",
    "dose_min_IN = demo[demo['route']==1]['dose_min'].mean()\n",
    "dose_min_IV = demo[demo['route']==2]['dose_min'].mean()\n",
    "dose_min_IM = demo[demo['route']==3]['dose_min'].mean()\n",
    "# Doses max\n",
    "dose_max_IN = demo[demo['route']==1]['dose_max'].mean()\n",
    "dose_max_IV = demo[demo['route']==2]['dose_max'].mean()\n",
    "dose_max_IM = demo[demo['route']==3]['dose_max'].mean()\n",
    "\n",
    "print(\"Total N = %d\" % len(ages))\n",
    "print(\"Sexes: %d male, %d female\" % (num_male, num_female))\n",
    "print(\"Ages: range %d to %d (Mean %f, STD %f).\" % (np.min(ages), np.max(ages), np.mean(ages), np.std(ages)))\n",
    "print(\"Route of admin: %d IN, %d IV, %d IM\" % (num_IN, num_IV, num_IM))\n",
    "print(\"Avg min dose: %f (IN), %f (IV), %f (IM)\" % (dose_min_IN, dose_min_IV, dose_min_IM))\n",
    "print(\"Avg max dose: %f (IN), %f (IV), %f (IM)\" % (dose_max_IN, dose_max_IV, dose_max_IM)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0b9dd",
   "metadata": {},
   "source": [
    "## Average PHQ-9 sum scores across induction sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average sum scores across session\n",
    "mus = []\n",
    "sems = []\n",
    "for sessionIdx in np.arange(1, 9):\n",
    "    d_session = d_phq[d_phq['sessionNumber']==sessionIdx]\n",
    "    mus.append(d_session['phqtotal'].mean())\n",
    "    sems.append(d_session['phqtotal'].sem())\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.errorbar(np.arange(1, 9), mus, yerr=sems, color='k', marker='o', linestyle='-')\n",
    "plt.xlabel('Ketamine session', fontsize=13)\n",
    "plt.ylabel('Average PHQ-9 sum score', fontsize=13)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'avgPHQ9sumScores.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "# Repeated measures ANOVA (only patients with complete data)\n",
    "d_full = pd.DataFrame(columns=['ID', 'sessionNumber', 'phqtotal'])\n",
    "ix = 0\n",
    "for sid in unique_patient_ids:\n",
    "    d_subject = d_phq[d_phq['ID']==sid]\n",
    "    sessions = np.sort(d_subject['sessionNumber'].values)\n",
    "    induction_sessions = [s for s in sessions if s <= 8]\n",
    "    if len(induction_sessions) == 8:  # subjects with exactly 8 repeats of PHQ-9 total score\n",
    "        for sessionNum in np.arange(1, 9):\n",
    "            sumscore = d_subject[d_subject['sessionNumber']==sessionNum]['phqtotal'].values[0]\n",
    "            d_full.loc[ix] = [sid, sessionNum, sumscore]\n",
    "            ix += 1\n",
    "sid_full = d_full['ID'].unique()\n",
    "print(\"%d of %d subjects have complete sum score data\" % (len(sid_full), len(unique_patient_ids)))\n",
    "friedmanRes = pg.friedman(data=d_full, dv=\"phqtotal\", within=\"sessionNumber\", subject=\"ID\")\n",
    "print(friedmanRes)\n",
    "\n",
    "# Check for normality of sum scores at each session\n",
    "# for sessionNum in np.arange(1, 9):\n",
    "#     sessionSumScores = d_full[d_full['sessionNumber']==sessionNum]['phqtotal'].values\n",
    "#     plt.figure()\n",
    "#     plt.title(str(sessionNum))\n",
    "#     plt.hist(sessionSumScores)\n",
    "#     print(\"Session %d\" % sessionNum)\n",
    "#     print(stats.shapiro(sessionSumScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa09d8",
   "metadata": {},
   "source": [
    "## Average PHQ-9 item scores across induction sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average PHQ-9 item scores across participants at the first 8 sessions\n",
    "phq_items = ['Anhedonia', 'Depressed', 'Sleep', 'Tired', 'Eating', 'Failure', 'Concentration', 'Moving', 'Self harm']\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ls = []\n",
    "colors = plt.cm.rainbow(np.linspace(0,1,9))\n",
    "all_slopes_phq = []\n",
    "for item_idx in np.arange(1, 10):\n",
    "    item_name = 'phqitem' + str(item_idx)\n",
    "    mus = []\n",
    "    sems = []\n",
    "    for sessionIdx in np.arange(1, 9):\n",
    "        d_session = d_phq[d_phq['sessionNumber']==sessionIdx]\n",
    "        mus.append(d_session[item_name].mean())\n",
    "        sems.append(d_session[item_name].sem())\n",
    "    l = ax.errorbar(np.arange(1, 9), mus, yerr=sems, color=colors[item_idx-1], alpha=0.9, marker='o', \n",
    "                    linestyle='-', label=phq_items[item_idx-1])\n",
    "    ls.append(l)\n",
    "    slope, intercept, r, p, se = stats.linregress(np.arange(1, 9), mus)\n",
    "    all_slopes_phq.append(slope)\n",
    "plt.legend(frameon=False, bbox_to_anchor=(0.5, 0.5, 0.9, 0.5))\n",
    "plt.xlabel('Ketamine session', fontsize=13)\n",
    "plt.ylabel('Average PHQ-9 item score', fontsize=13)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'avgPHQ9itemScores.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "# Test for significant change in item response across sessions\n",
    "for item_num in np.arange(1, 9):\n",
    "    item = 'phqitem' + str(item_num)\n",
    "    d_full = pd.DataFrame(columns=['ID', 'sessionNumber', 'item_response'])\n",
    "    ix = 0\n",
    "    for sid in unique_patient_ids:\n",
    "        d_subject = d_phq[d_phq['ID']==sid][[item, 'sessionNumber']]\n",
    "        sessions = np.sort(d_subject['sessionNumber'].values)\n",
    "        induction_sessions = [s for s in sessions if s <= 8]\n",
    "        if len(induction_sessions) == 8:  # subjects with exactly 8 repeats of this item\n",
    "            for sessionNum in np.arange(1, 9):\n",
    "                item_score = d_subject[d_subject['sessionNumber']==sessionNum][item].values[0]\n",
    "                d_full.loc[ix] = [sid, sessionNum, item_score]\n",
    "                ix += 1\n",
    "    sid_full = d_full['ID'].unique()\n",
    "    print(\"\\nITEM #%d: %d of %d subjects have complete data\" % (item_num, len(sid_full), len(unique_patient_ids)))\n",
    "    \n",
    "    friedmanRes = pg.friedman(data=d_full, dv=\"item_response\", within=\"sessionNumber\", subject=\"ID\")\n",
    "    print(friedmanRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea61cd",
   "metadata": {},
   "source": [
    "## Compare linear and exponential fits to item trajectories for individual patients\n",
    "Note, if you want to see all of the fits for every item and every patient, change the show_all_fits variable to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_fits = False # Set to True to plot all of the item fits for every patietnt\n",
    "\n",
    "# k = number of parameters in each model, including implicit parameter for error variance\n",
    "k_exp = 4\n",
    "k_lin = 3\n",
    "\n",
    "df_slopes = pd.DataFrame(columns=['id', 'item', 'slope_exp', 'slope_lin', 'rss_exp', 'rss_lin'])\n",
    "df_info = pd.DataFrame(columns=['id', 'item', 'aic_lin', 'bic_lin', 'aic_exp', 'bic_exp'])\n",
    "df_idx = 0\n",
    "for patient_idx, patient_id in enumerate(valid_phq_subjects):\n",
    "    d_patient = d_phq[d_phq['ID']==patient_id]\n",
    "    for item_number in np.arange(1, 10):\n",
    "        xs, ys = get_indiv_item_curve(d_patient, item_number, questionnaire='phq')\n",
    "        n = len(ys)\n",
    "        \n",
    "        # Fit linear model\n",
    "        init_slope = (ys[-1] - ys[0]) / (xs[-1] - xs[0])\n",
    "        init_intercept = ys[0]\n",
    "        res = minimize(rss_linear, x0=[init_slope, init_intercept], args=(np.array(xs), np.array(ys)))\n",
    "        slope, intercept = res.x\n",
    "        rss_lin = res.fun\n",
    "        \n",
    "        # Fit exponential model       \n",
    "        init_offset = ys[-1]\n",
    "        res = minimize(rss_exponential, x0=[1, 1, init_offset], args=(np.array(xs), np.array(ys)))\n",
    "        a, b, c = res.x \n",
    "        rss_exp = res.fun\n",
    "        \n",
    "        # Plot both models overlaid on data\n",
    "        title = \"Patient %d, Item %d\" % (patient_id, item_number)\n",
    "        if show_all_fits:\n",
    "            plot_both_lin_exp(xs, ys, slope, intercept, a, b, c, title=title)\n",
    "        if (patient_id == 17) and (item_number == 1):\n",
    "            plot_both_lin_exp(xs, ys, slope, intercept, a, b, c, title=title)\n",
    "            plt.savefig(fname=outdir+'egItemFitsPHQ9.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "        \n",
    "        # Compute AIC and BIC for the linear and exponential fits\n",
    "        n = d_patient.shape[0]\n",
    "        if rss_lin == 0:\n",
    "            aic_lin = -np.inf\n",
    "            bic_lin = -np.inf\n",
    "        else:\n",
    "            aic_lin = 2 * k_lin + n * np.log(rss_lin / n)\n",
    "            bic_lin = np.log(n) * k_lin + n * np.log(rss_lin / n)\n",
    "        if rss_exp == 0:\n",
    "            aic_exp = -np.inf\n",
    "            bic_exp = -np.inf\n",
    "        else:\n",
    "            aic_exp = 2 * k_exp + n * np.log(rss_exp / n)\n",
    "            bic_exp = np.log(n) * k_exp + n * np.log(rss_exp / n)\n",
    "\n",
    "        # Store results in dataframes\n",
    "        df_info.loc[df_idx] = [patient_id, item_number, aic_lin, bic_lin, aic_exp, bic_exp]\n",
    "        df_slopes.loc[df_idx] = [patient_id, item_number, b, slope, rss_exp, rss_lin]\n",
    "        df_idx += 1\n",
    "        \n",
    "# Compute how many patients were better fit overall by linear or exponential fits, where better fit\n",
    "# is defined as the majority of items having lower AIC or BIC\n",
    "aic_exp_better = 0\n",
    "aic_lin_better = 0\n",
    "bic_exp_better = 0\n",
    "bic_lin_better = 0\n",
    "itemwise_aic_lin_better = 0\n",
    "itemwise_aic_exp_better = 0\n",
    "itemwise_bic_lin_better = 0\n",
    "itemwise_bic_exp_better = 0\n",
    "for patient_idx, patient_id in enumerate(valid_phq_subjects):\n",
    "    df = df_info[df_info['id']==patient_id]\n",
    "    num_items = len(df['item'].unique())\n",
    "    num_aic_lin_better = sum(df['aic_lin'] < df['aic_exp'])\n",
    "    num_bic_lin_better = sum(df['bic_lin'] < df['bic_exp'])\n",
    "    itemwise_aic_lin_better += num_aic_lin_better\n",
    "    itemwise_aic_exp_better += num_items - num_aic_lin_better\n",
    "    itemwise_bic_lin_better += num_bic_lin_better\n",
    "    itemwise_bic_exp_better += num_items - num_bic_lin_better\n",
    "    if num_aic_lin_better >= 5:\n",
    "        aic_lin_better +=1\n",
    "    else:\n",
    "        aic_exp_better += 1\n",
    "    if num_bic_lin_better >= 5:\n",
    "        bic_lin_better +=1\n",
    "    else:\n",
    "        bic_exp_better += 1\n",
    "print(\"AIC: %d patients better with linear, %d patients better with exponential\" % (aic_lin_better, aic_exp_better))\n",
    "print(\"BIC: %d patients better with linear, %d patients better with exponential\" % (bic_lin_better, bic_exp_better))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b782ad",
   "metadata": {},
   "source": [
    "## Plot overall number of item comparisons that are better with exponential or linear fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "ax = plt.subplot(111)\n",
    "plt.bar([1, 3], [itemwise_aic_lin_better, itemwise_bic_lin_better], color='g', alpha=0.3)\n",
    "plt.bar([2, 4], [itemwise_aic_exp_better, itemwise_bic_exp_better], color='b', alpha=0.3)\n",
    "plt.xticks(np.arange(1, 5), ['Linear better', 'Exponential btter', 'Linear better', 'Exponential better'])\n",
    "plt.ylabel('Number of item comparisons', fontsize=14)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'AIC_BIC_PHQ9.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d5a4b",
   "metadata": {},
   "source": [
    "## Get linear slopes for every patient's item response trajectories\n",
    "This cell directly extracts the linear slopes for each questionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bb131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the slopes dataframes to store fitted slopes\n",
    "df_slopes = pd.DataFrame(columns=['id', 'item', 'slope_lin'])\n",
    "df_idx = 0\n",
    "for patient_idx, patient_id in enumerate(valid_phq_subjects):\n",
    "    d_patient = d_phq[d_phq['ID']==patient_id]\n",
    "    # PHQ fits\n",
    "    for item_number in np.arange(1, 10): \n",
    "        xs, ys = get_indiv_item_curve(d_patient, item_number, questionnaire='phq')\n",
    "        init_slope = (ys[-1] - ys[0]) / (xs[-1] - xs[0])\n",
    "        init_intercept = ys[0]\n",
    "        res = minimize(rss_linear, x0=[init_slope, init_intercept], args=(np.array(xs), np.array(ys)))\n",
    "        slope, intercept = res.x\n",
    "        df_slopes.loc[df_idx] = [patient_id, item_number, slope]\n",
    "        df_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915eff75",
   "metadata": {},
   "source": [
    "## PHQ-9:  Plot average slopes across patients for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872355ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "phq_items = ['Anhedonia', 'Depressed', 'Sleep', 'Tired', 'Eating', 'Failure', 'Concentration', 'Moving', 'Self harm']\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "all_slopes = []\n",
    "avg_slopes = []\n",
    "sem_slopes = []\n",
    "for item_number in np.arange(1, 10):\n",
    "    item_slopes = df_slopes[df_slopes['item']==item_number]['slope_lin']\n",
    "    all_slopes.append(item_slopes.values)\n",
    "    avg_slopes.append(item_slopes.mean())\n",
    "    ci95 = 1.96 * np.std(item_slopes) / math.sqrt(item_slopes.shape[0])\n",
    "    sem_slopes.append(stats.sem(item_slopes))\n",
    "tuples = list(zip(phq_items, avg_slopes, sem_slopes))\n",
    "tuples.sort(key = lambda x: x[1])\n",
    "sorted_item_names = [t[0] for t in tuples]\n",
    "ys = [t[1] for t in tuples]\n",
    "yerrs = [t[2] for t in tuples]\n",
    "ax.errorbar(np.arange(1, 10), ys, yerr=yerrs, marker='o', color='k')\n",
    "plt.xticks(np.arange(1, 10), sorted_item_names, rotation=45)\n",
    "plt.ylabel('Average linear slope', fontsize=12)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'avgSlopesPHQ9.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "# Friedman test for repeated measures\n",
    "print(pg.friedman(data=df_slopes, dv=\"slope_lin\", within=\"item\", subject=\"id\"))\n",
    "\n",
    "# Post hoc tests\n",
    "num_comparisons = 9 * 8 / 2  # k(k-1)/2\n",
    "bonferroni = 0.05 / num_comparisons\n",
    "print(bonferroni)\n",
    "for i in np.arange(1, 10):\n",
    "    for j in np.arange(1, 10):\n",
    "        if i == j:\n",
    "            continue\n",
    "        slopes_i = df_slopes[df_slopes['item']==i]['slope_lin'].values\n",
    "        slopes_j = df_slopes[df_slopes['item']==j]['slope_lin'].values\n",
    "        stat, p = stats.wilcoxon(slopes_i, slopes_j)\n",
    "        effect = (np.median(slopes_i) - np.median(slopes_j)) / np.mean([stats.iqr(slopes_i), stats.iqr(slopes_j)])\n",
    "        if p < bonferroni:\n",
    "            print(\"Items %d and %d are significantly different\" % (i, j))\n",
    "            print(\"p = %f\" % p)\n",
    "            print(\"effect = %f\" % effect)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454d72c",
   "metadata": {},
   "source": [
    "## PCA in nine-dimensional space of PHQ-9 slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60815076",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=9)\n",
    "\n",
    "# Create an n x m matrix, where n is number of items, and m is number of patients\n",
    "all_item_slopes = []\n",
    "for item_num in np.arange(1, 10):\n",
    "    # Slopes for each patient\n",
    "    this_item_data = df_slopes[(df_slopes['item']==item_num)]\n",
    "    item_slopes = this_item_data['slope_lin'].values\n",
    "    item_sids = this_item_data['id'].values\n",
    "    for i, sid in enumerate(item_sids):\n",
    "        assert(sid == valid_phq_subjects[i])\n",
    "    all_item_slopes.append(item_slopes) \n",
    "X = np.array(all_item_slopes).T\n",
    "\n",
    "# Scale data by mean prior to computing PCA\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "pca.fit(X)\n",
    "proj = pca.transform(X)\n",
    "\n",
    "# Separate colormap\n",
    "colors = []\n",
    "for proj1 in proj[:, 0]:\n",
    "    if proj1 < 0:\n",
    "        colors.append('g')\n",
    "    else:\n",
    "        colors.append('r')\n",
    "\n",
    "proj_responders = []\n",
    "proj_nonresponders = []\n",
    "for i in range(proj.shape[0]):\n",
    "    proj1 = proj[i, 0]\n",
    "    proj2 = proj[i, 1]\n",
    "    if proj1 < 0:\n",
    "        proj_responders.append([proj1, proj2])\n",
    "    else:\n",
    "        proj_nonresponders.append([proj1, proj2]) \n",
    "proj_responders = np.array(proj_responders)\n",
    "proj_nonresponders = np.array(proj_nonresponders)\n",
    "\n",
    "# Scree plot\n",
    "varexp = pca.explained_variance_ratio_\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(np.arange(1, 10), varexp, linestyle='--', marker='o', color='k')\n",
    "plt.title('Linear fits')\n",
    "plt.xlabel('Principle component', fontsize=12)\n",
    "plt.ylabel('Fraction variance explained', fontsize=12)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'screePHQ9.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "# Projection into first 2 PCs\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.scatter(proj_responders[:, 0], proj_responders[:, 1], marker='o', color = 'g', alpha=0.4, label='Responders')\n",
    "plt.scatter(proj_nonresponders[:, 0], proj_nonresponders[:, 1], marker='o', color = 'r', alpha=0.4, label='Non-responders')\n",
    "plt.xlabel('PC1', fontsize=12)\n",
    "plt.ylabel('PC2', fontsize=12)\n",
    "# plt.xlim([-5.5, 5])\n",
    "# plt.ylim([-3, 3])\n",
    "plt.axhline(y=0, color='grey', linestyle='--', alpha=0.7)\n",
    "plt.axvline(x=0, color='grey', linestyle='--', alpha=0.7)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.legend(loc='upper left')\n",
    "# plt.xlim([-5, 5])\n",
    "# plt.ylim([-5, 5])\n",
    "plt.savefig(fname=outdir+'pcaProjPHQ9.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "# Coefficients for each item of the PCs\n",
    "tuples = list(zip(phq_items, pca.components_[0, :], pca.components_[1, :]))\n",
    "\n",
    "# PC1 coefficients\n",
    "tuples.sort(key = lambda x: x[1], reverse=True)\n",
    "sorted_item_names = [t[0] for t in tuples]\n",
    "coefs = [t[1] for t in tuples]\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(range(len(coefs)), coefs, marker='o', linestyle='--', color='k')\n",
    "plt.ylabel('PC1 coefficient', fontsize=12)\n",
    "plt.xlabel('PHQ-9 Item', fontsize=12)\n",
    "plt.xticks(range(len(coefs)), sorted_item_names, rotation=60)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'pc1CoefPHQ9.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "# PC2 coefficients\n",
    "tuples = list(zip(phq_items, pca.components_[0, :], pca.components_[1, :]))\n",
    "tuples.sort(key = lambda x: x[2], reverse=True)\n",
    "sorted_item_names = [t[0] for t in tuples]\n",
    "coefs = [t[2] for t in tuples]\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(range(len(coefs)), coefs, marker='o', linestyle='--', color='k')\n",
    "plt.ylabel('PC2 coefficient', fontsize=12)\n",
    "plt.xlabel('PHQ-9 Item', fontsize=12)\n",
    "plt.xticks(range(len(coefs)), sorted_item_names, rotation=60)\n",
    "ax.axhline(y=0, linestyle='--', color='grey', alpha=0.5)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'pc2CoefPHQ9.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cdfff7",
   "metadata": {},
   "source": [
    "## PHQ: Which PCs are significant via parallel method\n",
    "Run the appropriate PCA (slopes, drops/raises, etc) before running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate significance of PCs by comparing to distribution of randomly generated data\n",
    "n = 10000\n",
    "percentile = 0.95\n",
    "all_eigs = []\n",
    "slope_min = df_slopes['slope_lin'].min()\n",
    "slope_max = df_slopes['slope_lin'].max()\n",
    "for i in range(n):\n",
    "    rand = np.random.choice(np.random.uniform([slope_min, slope_max]), X.shape) # random item response matrix\n",
    "    scaler = preprocessing.StandardScaler().fit(rand)\n",
    "    rand_scaled = scaler.transform(rand)\n",
    "    pca_rand = PCA(n_components=9).fit(rand_scaled)\n",
    "    all_eigs.append(pca_rand.explained_variance_) # explained_variance field are the eigenvalues\n",
    "all_eigs = np.array(all_eigs)\n",
    "all_eigs.sort(axis=0)\n",
    "\n",
    "print(\"\\n95th percentile of randomly generated eigenvalues:\")\n",
    "print(all_eigs[int(percentile * n), :]) # print the 95th percentile eigenvalue for each column\n",
    "print('\\n')\n",
    "print(\"Actual eigenvalues of the data:\")\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c6845",
   "metadata": {},
   "source": [
    "## Responders vs Non-responders: Average PHQ-9 sum score trajectories\n",
    "Here responder is defined as negative coefficient of first PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHQ-9 data\n",
    "firstPC = proj[:, 0]\n",
    "responder_idx = np.where(firstPC < 0)[0]\n",
    "nonresponder_idx = np.where(firstPC >= 0)[0]\n",
    "# SIDs\n",
    "subjects = np.array(valid_phq_subjects)\n",
    "responder_sids = subjects[responder_idx]\n",
    "nonresponder_sids = subjects[nonresponder_idx]\n",
    "print('%d responders, %d non-responders' % (len(responder_sids), len(nonresponder_sids)))\n",
    "\n",
    "# Data\n",
    "responder_data = d_phq[d_phq['ID'].isin(responder_sids)]\n",
    "nonresponder_data = d_phq[d_phq['ID'].isin(nonresponder_sids)]\n",
    "\n",
    "# Plot average PHQ-9 total scores across participants at the first 8 sessions\n",
    "mus_r = []\n",
    "mus_nr = []\n",
    "sems_r = []\n",
    "sems_nr = []\n",
    "for sessionIdx in np.arange(1, 9):\n",
    "    # Responders\n",
    "    d_session = responder_data[responder_data['sessionNumber']==sessionIdx]\n",
    "    mus_r.append(d_session['phqtotal'].mean())\n",
    "    sems_r.append(d_session['phqtotal'].sem())\n",
    "    # Nonresponders\n",
    "    d_session = nonresponder_data[nonresponder_data['sessionNumber']==sessionIdx]\n",
    "    mus_nr.append(d_session['phqtotal'].mean())\n",
    "    sems_nr.append(d_session['phqtotal'].sem())\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.errorbar(np.arange(1, 9), mus_r, yerr=sems_r, color='g', marker='o', linestyle='-', label='Responders')\n",
    "ax.errorbar(np.arange(1, 9), mus_nr, yerr=sems_nr, color='r', marker='o', linestyle='-', label='Non-responders')\n",
    "plt.xlabel('Ketamine session', fontsize=13)\n",
    "plt.ylabel('Average PHQ-9 sum score', fontsize=13)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.legend()\n",
    "plt.savefig(fname=outdir+'sumScoresByResponder.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# RM ANOVAs for change in responders and non-responders\n",
    "d_full = pd.DataFrame(columns=['ID', 'group', 'sessionNumber', 'phqtotal'])\n",
    "ix = 0\n",
    "for sid in unique_patient_ids:\n",
    "    d_subject = d_phq[d_phq['ID']==sid]\n",
    "    if sid in responder_sids:\n",
    "        group = 'responder'\n",
    "    elif sid in nonresponder_sids:\n",
    "        group = 'nonresponder'\n",
    "    else:\n",
    "        print('Undefined patient')\n",
    "        continue\n",
    "    sessions = np.sort(d_subject['sessionNumber'].values)\n",
    "    induction_sessions = [s for s in sessions if s <= 8]\n",
    "    if len(induction_sessions) == 8:  # subjects with exactly 8 repeats of PHQ-9 total score\n",
    "        for sessionNum in np.arange(1, 9):\n",
    "            sumscore = d_subject[d_subject['sessionNumber']==sessionNum]['phqtotal'].values[0]\n",
    "            d_full.loc[ix] = [sid, group, sessionNum, sumscore]\n",
    "            ix += 1\n",
    "            \n",
    "dfull_responders = d_full[d_full['group']=='responder']  \n",
    "dfull_nonresponders = d_full[d_full['group']=='nonresponder']  \n",
    "sid_full_responder = dfull_responders['ID'].unique()\n",
    "sid_full_nonresponder = dfull_nonresponders['ID'].unique()\n",
    "\n",
    "print(\"%d of %d responders have complete sum score data\" % (len(sid_full_responder), len(responder_sids)))\n",
    "print(\"%d of %d non-responders have complete sum score data\" % (len(sid_full_nonresponder), len(nonresponder_sids)))\n",
    "\n",
    "print(\"\\n---Responder ANOVA---\")\n",
    "aov = pg.rm_anova(dv='phqtotal', within='sessionNumber', subject='ID',\n",
    "                  data=dfull_responders, detailed=True, effsize=\"np2\")\n",
    "print(aov)\n",
    "\n",
    "print(\"\\n---Non-responder ANOVA---\")\n",
    "aov = pg.rm_anova(dv='phqtotal', within='sessionNumber', subject='ID',\n",
    "                  data=dfull_nonresponders, detailed=True, effsize=\"np2\")\n",
    "print(aov)\n",
    "\n",
    "\n",
    "# Compute mean changes from baseline to final session for the responders and non-responders\n",
    "deltas_responders = []\n",
    "deltas_nonresponders = []\n",
    "for sid in sid_full_responder:\n",
    "    d = d_full[d_full['group']=='responder']\n",
    "    baseline = d[(d['ID']==sid) & (d['sessionNumber']==1)]['phqtotal'].values[0]\n",
    "    final = d[(d['ID']==sid) & (d['sessionNumber']==8)]['phqtotal'].values[0]\n",
    "    deltas_responders.append(final - baseline)\n",
    "for sid in sid_full_nonresponder:\n",
    "    d = d_full[d_full['group']=='nonresponder']\n",
    "    baseline = d[(d['ID']==sid) & (d['sessionNumber']==1)]['phqtotal'].values[0]\n",
    "    final = d[(d['ID']==sid) & (d['sessionNumber']==8)]['phqtotal'].values[0]\n",
    "    deltas_nonresponders.append(final - baseline)\n",
    "mu_r = np.mean(deltas_responders)\n",
    "ci95_r = 1.96 * stats.sem(deltas_responders)\n",
    "mu_nr = np.mean(deltas_nonresponders)\n",
    "ci95_nr = 1.96 * stats.sem(deltas_nonresponders)\n",
    "print(\"\\nResponders: Mean change from baseline to final session: %f (CI95: %f to %f)\" % \n",
    "      (mu_r, mu_r - ci95_r, mu_r + ci95_r))\n",
    "print(\"Non-responders: Mean change from baseline to final session: %f (CI95: %f to %f)\" % \n",
    "      (mu_nr, mu_nr - ci95_nr, mu_nr + ci95_nr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1bc1c",
   "metadata": {},
   "source": [
    "## PC2 Affective vs. Somatic: Sum-score trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ea884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affectiveSums(df):\n",
    "    affective_items = ['phqitem' + str(i) for i in [1, 2, 6, 9]]\n",
    "    all_sums = []\n",
    "    for index, row in df.iterrows():\n",
    "        item_vals = row[affective_items].values\n",
    "        all_sums.append(sum(item_vals))\n",
    "    return all_sums\n",
    "    \n",
    "\n",
    "def somaticSums(df):\n",
    "    somatic_items = ['phqitem' + str(i) for i in [3, 4, 5, 7, 8]]\n",
    "    all_sums = []\n",
    "    for index, row in df.iterrows():\n",
    "        item_vals = row[somatic_items].values\n",
    "        all_sums.append(sum(item_vals))\n",
    "    return all_sums\n",
    "\n",
    "\n",
    "# PHQ-9 data\n",
    "firstPC = proj[:, 0]\n",
    "responder_idx = np.where(firstPC < 0)[0]\n",
    "nonresponder_idx = np.where(firstPC >= 0)[0]\n",
    "\n",
    "secondPC = proj[:, 1]\n",
    "affective_idx = np.where(secondPC < 0)[0]\n",
    "somatic_idx = np.where(secondPC >= 0)[0]\n",
    "\n",
    "\n",
    "# SIDs\n",
    "subjects = np.array(valid_phq_subjects)\n",
    "responder_sids = subjects[responder_idx]\n",
    "nonresponder_sids = subjects[nonresponder_idx]\n",
    "affective_sids = subjects[affective_idx]\n",
    "somatic_sids = subjects[somatic_idx]\n",
    "r_affective_sids = list(set(responder_sids).intersection(affective_sids))\n",
    "r_somatic_sids = list(set(responder_sids).intersection(somatic_sids))\n",
    "\n",
    "print('%d responders, %d non-responders' % (len(responder_sids), len(nonresponder_sids)))\n",
    "print('%d affective-dominant responders, %d somatic-dominant responders' % \n",
    "      (len(r_affective_sids), len(r_somatic_sids)))\n",
    "\n",
    "\n",
    "# Responder data (somatic and affetive break-outs)\n",
    "r_affective = d_phq[d_phq['ID'].isin(r_affective_sids)]\n",
    "r_somatic = d_phq[d_phq['ID'].isin(r_somatic_sids)]\n",
    "\n",
    "\n",
    "# -- Responders Only -- \n",
    "# Plot trajectories of sub-sum-scores for affetive and somatic symptom clusters\n",
    "# separately broken out by responder-affetive and responder-somatic groups\n",
    "\n",
    "mus_AR_affective = []\n",
    "mus_SR_affective = []\n",
    "mus_AR_somatic = []\n",
    "mus_SR_somatic = []\n",
    "sems_AR_affective = []\n",
    "sems_SR_affective = []\n",
    "sems_AR_somatic = []\n",
    "sems_SR_somatic = []\n",
    "for sessionIdx in np.arange(1, 9):\n",
    "    # Get data for this session, split by affective- and somatic-dominant response\n",
    "    d_AR = r_affective[r_affective['sessionNumber']==sessionIdx]\n",
    "    d_SR = r_somatic[r_somatic['sessionNumber']==sessionIdx]\n",
    "    \n",
    "    # Affective dominant responders: split by affective and somatic symptom clusters\n",
    "    # --> Affective cluster response\n",
    "    affectiveSumScores = affectiveSums(d_AR)\n",
    "    mus_AR_affective.append(np.mean(affectiveSumScores))\n",
    "    sems_AR_affective.append(stats.sem(affectiveSumScores))\n",
    "    # --> Somatic cluster response\n",
    "    somaticSumScores = somaticSums(d_AR)\n",
    "    mus_AR_somatic.append(np.mean(somaticSumScores))\n",
    "    sems_AR_somatic.append(stats.sem(somaticSumScores))\n",
    "    \n",
    "    # Somatic dominant responders: split by affective and somatic symptom clusters\n",
    "    # --> Affective cluster response\n",
    "    affectiveSumScores = affectiveSums(d_SR)\n",
    "    mus_SR_affective.append(np.mean(affectiveSumScores))\n",
    "    sems_SR_affective.append(stats.sem(affectiveSumScores))\n",
    "    # --> Somatic cluster response\n",
    "    somaticSumScores = somaticSums(d_SR)\n",
    "    mus_SR_somatic.append(np.mean(somaticSumScores))\n",
    "    sems_SR_somatic.append(stats.sem(somaticSumScores))\n",
    "    \n",
    "    \n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.errorbar(np.arange(1, 9), mus_AR_affective, yerr=sems_AR_affective, color='k', marker='o', linestyle='-', label='Negative PC2')\n",
    "ax.errorbar(np.arange(1, 9), mus_SR_affective, yerr=sems_SR_affective, color='k', marker='x', linestyle='--', label='Non-negative PC2')\n",
    "plt.xlabel('Ketamine session', fontsize=13)\n",
    "plt.ylabel('Affective subdomain score', fontsize=13)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.legend()\n",
    "plt.savefig(fname=outdir+'affectiveClusterSumScores.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.errorbar(np.arange(1, 9), mus_AR_somatic, yerr=sems_AR_somatic, color='k', marker='o', linestyle='-', label='Negative PC2')\n",
    "ax.errorbar(np.arange(1, 9), mus_SR_somatic, yerr=sems_SR_somatic, color='k', marker='x', linestyle='--', label='Non-negative PC2')\n",
    "plt.xlabel('Ketamine session', fontsize=13)\n",
    "plt.ylabel('Somatic subdomain score', fontsize=13)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.legend()\n",
    "plt.savefig(fname=outdir+'somaticClusterSumScores.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# RM ANOVAs\n",
    "d_full = pd.DataFrame(columns=['ID', 'group', 'sessionNumber', 'affective_total', 'somatic_total'])\n",
    "ix = 0\n",
    "for sid in unique_patient_ids:\n",
    "    d_subject = d_phq[d_phq['ID']==sid]\n",
    "    if sid in r_affective_sids:\n",
    "        group = 'AR'\n",
    "    elif sid in r_somatic_sids:\n",
    "        group = 'SR'\n",
    "    else:\n",
    "        group = 'NA'\n",
    "        continue\n",
    "    sessions = np.sort(d_subject['sessionNumber'].values)\n",
    "    induction_sessions = [s for s in sessions if s <= 8]\n",
    "    if len(induction_sessions) == 8:  # subjects with exactly 8 repeats of PHQ-9 total score\n",
    "        for sessionNum in np.arange(1, 9):\n",
    "            d_session = d_subject[d_subject['sessionNumber']==sessionNum]\n",
    "            affective_score = affectiveSums(d_session)\n",
    "            somatic_score = somaticSums(d_session)    \n",
    "            d_full.loc[ix] = [sid, group, sessionNum, affective_score[0], somatic_score[0]]\n",
    "            ix += 1\n",
    "            \n",
    "dfull_AR = d_full[d_full['group']=='AR']  \n",
    "dfull_SR = d_full[d_full['group']=='SR']  \n",
    "sid_full_AR = dfull_AR['ID'].unique()\n",
    "sid_full_SR = dfull_SR['ID'].unique()\n",
    "\n",
    "print(\"%d of %d affective-dominant responders have complete data\" % (len(sid_full_AR), len(r_affective_sids)))\n",
    "print(\"%d of %d somatic-dominant responders have complete data\" % (len(sid_full_SR), len(r_somatic_sids)))\n",
    "\n",
    "print(\"\\n---Affective cluster ANOVA---\")\n",
    "aov = pg.mixed_anova(dv='affective_total', within='sessionNumber', \n",
    "                     between='group', subject='ID', correction=True, data=d_full)\n",
    "print(aov)\n",
    "\n",
    "print(\"\\n---Somatic cluster ANOVA---\")\n",
    "aov = pg.mixed_anova(dv='somatic_total', within='sessionNumber', between='group', subject='ID', data=d_full)\n",
    "print(aov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc4c45",
   "metadata": {},
   "source": [
    "## Compute classical percent change in sum score (from baseline to last session) for responders and non-responders\n",
    "Responders and non-responders are defined, as above, by the sign of PC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative_sum = pd.DataFrame(columns=['id', 'rel_change'])\n",
    "df_idx = 0\n",
    "# PHQ-9\n",
    "for patient_idx, patient_id in enumerate(valid_phq_subjects):\n",
    "    d_patient = d_phq[d_phq['ID']==patient_id]\n",
    "    xs, ys = get_indiv_sum_curve(d_patient, questionnaire='phq')\n",
    "    rel_change = (ys[0] - ys[-1]) / ys[0]\n",
    "    df_relative_sum.loc[df_idx] = [patient_id, rel_change]\n",
    "    df_idx += 1\n",
    "\n",
    "# Evaluate the classical percent change for responders and non-responders \n",
    "r_change = df_relative_sum[df_relative_sum['id'].isin(responder_sids)]['rel_change'].values\n",
    "nr_change = df_relative_sum[df_relative_sum['id'].isin(nonresponder_sids)]['rel_change'].values\n",
    "print('responder avg (SEM) change: %f +/- %f' % (np.mean(r_change), stats.sem(r_change)))\n",
    "print('non-responder avg (SEM) change: %f +/- %f' % (np.mean(nr_change), stats.sem(nr_change)))\n",
    "\n",
    "print('responder median (IQR) change: %f +/- %f' % (np.median(r_change), stats.iqr(r_change)))\n",
    "print('non-responder median (IQR) change: %f +/- %f' % (np.median(nr_change), stats.iqr(nr_change)))\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "ax.axhline(y=0, linestyle='--', color='grey', alpha=0.5)\n",
    "vp = ax.violinplot([r_change, nr_change], positions=[1,2], showmeans=False, showmedians=True,\n",
    "                   widths=0.2)\n",
    "plt.xlim([0.5, 2.5])\n",
    "plt.xticks([1, 2], ['Positive PC1\\n(Responders)', 'Negative PC1\\n(Nonresponders)'])\n",
    "plt.ylabel('Relative improvement in PHQ-9', fontsize=12)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "parts = vp[\"bodies\"]\n",
    "colors=['green', 'red']\n",
    "for i in range(len(parts)):\n",
    "    parts[i].set_facecolor(colors[i])\n",
    "    parts[i].set_edgecolor('grey')\n",
    "    parts[i].set_edgecolor('grey')\n",
    "for partname in ('cbars','cmins','cmaxes','cmedians'):\n",
    "    part = vp[partname]\n",
    "    part.set_edgecolor('grey')\n",
    "    part.set_linewidth(1)\n",
    "plt.savefig(fname=outdir+'relativeImprovementByPC1.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d905783",
   "metadata": {},
   "source": [
    "## Logistic regression (with exhaustive feature selection) to predict response from baseline symtpoms\n",
    "Evaluate all 511 possible combinations of items as features, cross validating each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b689ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of all combinations of items to use as features\n",
    "all_combos = []\n",
    "for r in np.arange(1, 10):\n",
    "    combos = combinations(all_phq_item_cols, r)\n",
    "    for c in combos:\n",
    "        c = list(c)\n",
    "        all_combos.append(c)\n",
    "n_combos = len(all_combos)\n",
    "        \n",
    "# Baseline PHQ-9 for PHQ-9 responders and non-responders\n",
    "baseline_data = d_phq[d_phq['sessionNumber']==1].sort_values(by='ID')\n",
    "    \n",
    "# Loop over all feature sets, cross validating the prediction performance of each\n",
    "df_cv_all = pd.DataFrame(columns=['features', 'coefs', 'all_accuracy', 'accuracy_mean', 'accuracy_ci',\n",
    "                                  'precision', 'recall', 'f1'])\n",
    "idx = 0\n",
    "for i, c in enumerate(all_combos):\n",
    "    if i % 5 == 0:\n",
    "        print('processing model %d of %d' % (i, n_combos))\n",
    "    x = baseline_data[c].values\n",
    "    y = np.array([1 if p < 0 else 0 for p in proj[:, 0]])\n",
    "    # Fit model to raw data to get coefs\n",
    "    model = LogisticRegression().fit(x, y)\n",
    "    coefs = model.coef_[0]\n",
    "    # Cross validate to get model performance for this feature set\n",
    "    rkf = RepeatedKFold(n_splits=5, n_repeats=1000)\n",
    "    all_acc = []\n",
    "    all_conf = np.zeros((2,2))\n",
    "    for train, test in rkf.split(x):\n",
    "        x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
    "        model = LogisticRegression().fit(x_train, y_train)\n",
    "        all_acc.append(model.score(x_test, y_test))\n",
    "        y_pred = model.predict(x_test)\n",
    "        all_conf += confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[0, 1])\n",
    "    ci95 = 1.95 * np.std(all_acc) / math.sqrt(len(all_acc))\n",
    "    precision = all_conf[1][1] / (all_conf[1][1] + all_conf[0][1])\n",
    "    recall = all_conf[1][1] / (all_conf[1][1] + all_conf[1][0])\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    df_cv_all.loc[idx] = [c, list(coefs), all_acc, np.mean(all_acc), ci95, precision, recall, f1]\n",
    "    idx += 1\n",
    "    \n",
    "    if i > 1:\n",
    "        break\n",
    "    \n",
    "# Save the tuning results to disk\n",
    "headers = ['features', 'coefs', 'all_accuracy', 'accuracy_mean', 'accuracy_ci', 'precision', 'recall', 'f1']\n",
    "with open(preprocess_outdir + 'cvFeatureSelection.csv', 'wb') as outFile:\n",
    "    df_cv_all.to_csv(outFile, header=headers, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd1e5c",
   "metadata": {},
   "source": [
    "## Read exhaustive feature selection dataframe from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9439c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertFeatures(f, sep=', ', toNum=False):\n",
    "    f = f.strip(\"[]\").split(sep)\n",
    "    if toNum:\n",
    "        f = [float(fi.strip(\"\\'\")) for fi in f]\n",
    "    else:\n",
    "        f = [fi.strip(\"\\'\") for fi in f]\n",
    "    return f\n",
    "df_cv_all = pd.read_csv(preprocess_outdir + 'cvFeatureSelection.csv')\n",
    "df_cv_all['features'] = df_cv_all['features'].apply(convertFeatures, toNum=False)\n",
    "df_cv_all['coefs'] = df_cv_all['coefs'].apply(convertFeatures, toNum=True)\n",
    "df_cv_all['all_accuracy'] = df_cv_all['all_accuracy'].apply(convertFeatures, toNum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd3045",
   "metadata": {},
   "source": [
    "## Examine the best model from exhaustive feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model performances\n",
    "df_sorted = df_cv_all.sort_values(by='accuracy_mean', ascending=False)\n",
    "all_accs = df_sorted['accuracy_mean'].values\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.hist(df_sorted['accuracy_mean'].values, color='w', edgecolor='k')\n",
    "plt.xlabel('CV classification accuracy', fontsize=12)\n",
    "plt.ylabel('Number of models', fontsize=12)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'histAllModelMeanAcc.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "# How many are beter than chance?\n",
    "num_good = df_sorted[df_sorted['accuracy_mean'] > 0.5].shape[0]\n",
    "num_overall = df_sorted.shape[0]\n",
    "print(\"%d / %d (%f) models had better than chance CV classifiation accuracy\" % \n",
    "     (num_good, num_overall, num_good/num_overall))\n",
    "\n",
    "# Evaluate the \"best\" model\n",
    "best_model = df_sorted.iloc[0]\n",
    "print(\"----Model with best CV accuracy---\")\n",
    "print(best_model)\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.hist(best_model['all_accuracy'], color='w', edgecolor='k', bins=np.arange(0.3, 1.0, 0.05))\n",
    "plt.xlabel('CV classification accuracy', fontsize=12)\n",
    "plt.ylabel('Number of CV iterations', fontsize=12)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom') \n",
    "plt.savefig(fname=outdir+'histBestModelCVAcc.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad7018",
   "metadata": {},
   "source": [
    "## Analyze average baseline item responses between responders and non-responders\n",
    "Attempt to better understand the nature of differences that allow for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline PHQ-9 for PHQ-9 responders and non-responders\n",
    "baseline_data = d_phq[(d_phq['sessionNumber']==1)]\n",
    "for i, sid in enumerate (baseline_data['ID'].values):\n",
    "    assert(sid == valid_phq_subjects[i])\n",
    "\n",
    "x = baseline_data[all_phq_item_cols].values\n",
    "y = np.array([1 if p < 0 else 0 for p in proj[:, 0]])\n",
    "improved_idx = np.where(y == 1)[0]\n",
    "worsened_idx = np.where(y == 0)[0]\n",
    "df_baseline = pd.DataFrame(columns=['item_name', 'improved_means', 'improved_sems',\n",
    "                                    'worsened_means', 'worsened_sems',\n",
    "                                    'effect_size', 'p_value'])\n",
    "idx = 0\n",
    "for i, item in enumerate(phq_items):\n",
    "    baseline_item_vals = x[:, i]\n",
    "    improved_means = np.mean(baseline_item_vals[improved_idx])\n",
    "    worsened_means = np.mean(baseline_item_vals[worsened_idx])\n",
    "    improved_sems = stats.sem(baseline_item_vals[improved_idx])\n",
    "    worsened_sems = stats.sem(baseline_item_vals[worsened_idx])\n",
    "    g = pg.compute_effsize(baseline_item_vals[improved_idx],\n",
    "                                     baseline_item_vals[worsened_idx], eftype='CLES')\n",
    "    t, p = stats.ttest_ind(baseline_item_vals[improved_idx],\n",
    "                              baseline_item_vals[worsened_idx])\n",
    "    df_baseline.loc[idx] = [item, improved_means, improved_sems, worsened_means, worsened_sems, np.abs(g), p]\n",
    "    idx += 1\n",
    "df_baseline = df_baseline.sort_values(by='effect_size', ascending=False)\n",
    "print(\"\\nPHQ for PHQ response p-values\")\n",
    "print(df_baseline['p_value'].values)\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.subplot(111)\n",
    "xs1 = np.arange(len(phq_items))\n",
    "ax.errorbar(xs1, df_baseline['improved_means'], yerr=df_baseline['improved_sems'], linestyle='None', color='g', marker='o', label='Responder')\n",
    "ax.errorbar(xs1+0.2, df_baseline['worsened_means'], yerr=df_baseline['worsened_sems'], linestyle='None', color='r', marker='o', label='Non-responder')\n",
    "plt.xticks(xs1+0.1, df_baseline['item_name'], rotation=45)\n",
    "plt.ylabel('Average baseline response', fontsize=12)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.legend(bbox_to_anchor=(0, 0., 1.2, 1))\n",
    "plt.savefig(fname=outdir+'baselineItemsByResponder.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "\n",
    "# dpred_full = pd.DataFrame(columns=['id', 'responded', 'item', 'itemval'])\n",
    "# ix = 0\n",
    "# for i, sid in enumerate(d_phq['ID'].unique()):\n",
    "#     if i in improved_idx:\n",
    "#         responded = 1\n",
    "#     else:\n",
    "#         responded = 0\n",
    "#     for item in all_phq_item_cols:\n",
    "#         item_val = d_phq[(d_phq['ID']==sid) & (d_phq['sessionNumber']==1)][item].values[0]\n",
    "#         dpred_full.loc[ix] = [sid, responded, item, item_val]\n",
    "#         ix += 1\n",
    "\n",
    "# aov = pg.mixed_anova(dv='itemval', between='responded', within='item', subject='id', data=dpred_full)\n",
    "# print(aov.round(3))\n",
    "\n",
    "# aov = pg.pairwise_tests(data=dpred_full, dv='itemval', between='responded', within='item', subject='id')\n",
    "# print(aov.round(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa872a",
   "metadata": {},
   "source": [
    "## Exhaustive feature selection with threshold tuning\n",
    "Examine all 511 feature sets, as above, but now for each of these models try a full range of decision thresholds from prbability 0 to 1. Thus, we now are effectively evaluating thousands of models, cross validating each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phq_item_cols = ['phqitem' + str(i) for i in np.arange(1, 10)]\n",
    "baseline_data = d_phq[d_phq['sessionNumber']==1]\n",
    "for i, sid in enumerate(baseline_data['ID'].values):\n",
    "    assert(sid == valid_phq_subjects[i])\n",
    "    \n",
    "all_combos = []\n",
    "for r in np.arange(1, 10):\n",
    "    combos = combinations(all_phq_item_cols, r)\n",
    "    for c in combos:\n",
    "        c = list(c)\n",
    "        all_combos.append(c)\n",
    "n_combos = len(all_combos)\n",
    "\n",
    "df = pd.DataFrame(columns=['features', 'sensitivity', 'specificity', 'ppv', 'npv', 'acc', 'thresh'])\n",
    "ix = 0\n",
    "y = np.array([1 if p < 0 else 0 for p in proj[:, 0]])\n",
    "for i, c in enumerate(all_combos):\n",
    "    if i % 10 == 0:\n",
    "        print('processing model %d of %d' % (i, n_combos))\n",
    "    x = baseline_data[c].values\n",
    "    for decision_thresh in np.arange(0, 1, 0.05):\n",
    "        rkf = RepeatedKFold(n_splits=5, n_repeats=200)\n",
    "        conf = np.zeros((2,2))\n",
    "        for train, test in rkf.split(x):\n",
    "            x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
    "            model = LogisticRegression().fit(x_train, y_train)\n",
    "            probclass = model.predict_proba(x_test)\n",
    "            y_pred = []\n",
    "            for i in range(probclass.shape[0]):\n",
    "                prob0, prob1 = probclass[i, :]\n",
    "                if prob1 > decision_thresh:\n",
    "                    y_pred.append(1)\n",
    "                else:\n",
    "                    y_pred.append(0)\n",
    "            conf += confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[0, 1])\n",
    "        accuracy = (conf[0][0] + conf[1][1]) / np.sum(conf)\n",
    "        PPV = conf[1][1] / (conf[1][1] + conf[0][1])\n",
    "        NPV = conf[0][0] / (conf[0][0] + conf[1][0])\n",
    "        sensitivity = conf[1][1] / (conf[1][1] + conf[1][0])\n",
    "        specificity = conf[0][0] / (conf[0][0] + conf[0][1])\n",
    "        df.loc[ix] = [c, sensitivity, specificity, PPV, NPV, accuracy, decision_thresh]\n",
    "        ix += 1\n",
    "        \n",
    "        \n",
    " # Save the tuning results to disk\n",
    "headers = ['features', 'sensitivity', 'specificity', 'ppv', 'npv', 'acc', 'thresh']\n",
    "with open(preprocess_outdir + 'cvTuning.csv', 'wb') as outFile:\n",
    "    df.to_csv(outFile, header=headers, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb1e77",
   "metadata": {},
   "source": [
    "## Read threshold tuning dataframe from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f975a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertFeatures(f):\n",
    "    f = f.strip(\"[]\").split(\", \")\n",
    "#     print(f[0])\n",
    "    f = [fi.strip(\"\\'\") for fi in f]\n",
    "    return f\n",
    "df = pd.read_csv(preprocess_outdir + 'cvTuning.csv')\n",
    "df['features'] = df['features'].apply(convertFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00797e",
   "metadata": {},
   "source": [
    "## For different predictive values (PPV/NPV, x-axis), plot the best coverage (Sensitivity/Specificity) out of all models with that predicive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b628419",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = d_phq[d_phq['sessionNumber']==1]\n",
    "y = np.array([1 if p < 0 else 0 for p in proj[:, 0]])\n",
    "\n",
    "dfx = pd.DataFrame(columns=['type', 'target', 'ppv', 'npv', 'sensitivity', 'specificity'])\n",
    "ix = 0\n",
    "min_targets = np.arange(0.65, 1.0, 0.001)\n",
    "for mt in min_targets:\n",
    "    # Extract the top models from CV\n",
    "    top_neg_model = df[df['npv']>=mt].sort_values(by='specificity', ascending=False).iloc[0]\n",
    "    top_pos_model = df[df['ppv']>=mt].sort_values(by='sensitivity', ascending=False).iloc[0]\n",
    "    custom_cols_neg = top_neg_model.features\n",
    "    custom_cols_pos = top_pos_model.features\n",
    "    thresh_neg = top_neg_model.thresh\n",
    "    thresh_pos = top_pos_model.thresh\n",
    "    \n",
    "    # Negative model\n",
    "    dfx.loc[ix] = ['neg', mt, top_neg_model.ppv, top_neg_model.npv,\n",
    "                   top_neg_model.sensitivity, top_neg_model.specificity]\n",
    "    ix += 1\n",
    "\n",
    "    # Positive model\n",
    "    dfx.loc[ix] = ['pos', mt, top_pos_model.ppv, top_pos_model.npv,\n",
    "                   top_pos_model.sensitivity, top_pos_model.specificity]\n",
    "    ix += 1\n",
    "    \n",
    "dfp = dfx[dfx['type']=='pos'].sort_values(by='target')\n",
    "dfn = dfx[dfx['type']=='neg'].sort_values(by='target')\n",
    " \n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(dfp['ppv'], dfp['sensitivity'], color='g', marker='o', label='Predicting response')\n",
    "plt.plot(dfn['npv'], dfn['specificity'], color='r', marker='o', label='Predicting non-response')\n",
    "ax.axvline(x=0.90, linestyle='--', color='grey', alpha=0.3)\n",
    "ax.axhline(y=0.10, linestyle='--', color='grey', alpha=0.3)\n",
    "plt.xlabel('% correct predictions\\n (PPV/NPV)', fontsize=14)\n",
    "plt.ylabel('% candidates identified\\n (Sensitivity/Specificity)', fontsize=14)\n",
    "plt.ylim([0, 0.4])\n",
    "plt.legend()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'sens_vs_ppv_all.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc1aab",
   "metadata": {},
   "source": [
    "## Identify the best model for predicting non-response among those with NPV > 90% and specificity > 10%\n",
    "Arithmetic mean of NPV and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = d_phq[d_phq['sessionNumber']==1]\n",
    "y = np.array([1 if p < 0 else 0 for p in proj[:, 0]])\n",
    "\n",
    "# Negative models\n",
    "all_neg_models = df[(df['npv']>=0.9) & (df['specificity']>0.10)]\n",
    "all_neg_models['score'] = 0.5 * all_neg_models['npv'] + 0.5 * all_neg_models['specificity']\n",
    "top_neg_model = all_neg_models.sort_values(by='score', ascending=False).iloc[0]\n",
    "\n",
    "cols = top_neg_model.features\n",
    "x = baseline_data[cols].values\n",
    "model = LogisticRegression(penalty='none').fit(x, y)\n",
    "        \n",
    "print(\"Predicting NON-response\")\n",
    "print(\"PPV (CV): %f\" % top_neg_model.ppv)\n",
    "print(\"NPV (CV): %f\" % top_neg_model.npv)\n",
    "print(\"Sensitivity (CV): %f\" % top_neg_model.sensitivity)\n",
    "print(\"Specificity (CV): %f\" % top_neg_model.specificity)\n",
    "print(\"Accuracy (CV): %f\" % top_neg_model.acc)\n",
    "print('\\n')\n",
    "\n",
    "raw_coefs = list(zip(cols, model.coef_[0]))\n",
    "raw_coefs.sort(key=lambda x: x[1], reverse=True)\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(np.arange(len(raw_coefs)), [c[1] for c in raw_coefs], linestyle='None', marker='o', color='k')\n",
    "# plt.xticks(np.arange(len(raw_coefs)), ['Depressed Mood', 'Eating', 'Self-Harm'], rotation=50)\n",
    "ax.axhline(y=0, linestyle='--', color='grey', alpha=0.5)\n",
    "plt.title(\"Predict non-response\", fontsize=12)\n",
    "plt.xlabel('PHQ-9 item', fontsize=12)\n",
    "plt.ylabel('Regression coefficient', fontsize=12)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig(fname=outdir+'bestNegModelCoefs.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")\n",
    "\n",
    "print(\"Top negative model\")\n",
    "print(top_neg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea2a78",
   "metadata": {},
   "source": [
    "## Distribution of parameters across all good models for predicting non-response\n",
    "All the models meeting pre-specified performance criteria of > 90% NPV and > 10% specificity. Note that most models do not include all nine items as features; this is a distribution plot across all good models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04af28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = all_neg_models.sort_values(by='score', ascending=False)\n",
    "y = np.array([1 if p < 0 else 0 for p in proj[:, 0]])\n",
    "\n",
    "params = [[] for i in range(9)]\n",
    "for model_ix in range(ms.shape[0]):\n",
    "    m = ms.iloc[model_ix]\n",
    "    fs = m['features']\n",
    "    x = baseline_data[fs].values\n",
    "    model = LogisticRegression(penalty='none').fit(x, y)\n",
    "    for feature_ix, f in enumerate(fs):\n",
    "        param_val = model.coef_[0][feature_ix] \n",
    "        param_ix = int(f[-1]) - 1  # index from 0 - 8 of this param\n",
    "        params[param_ix].append(param_val)\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "vp = plt.violinplot(params)\n",
    "for pc in vp['bodies']:\n",
    "    pc.set_facecolor('white')\n",
    "    pc.set_edgecolor('black')\n",
    "for partname in ('cmaxes', 'cmins', 'cbars'):\n",
    "    vp[partname].set_edgecolor('black')\n",
    "ax.axhline(y=0, linestyle='--', color='grey', alpha=0.5)\n",
    "plt.xticks(np.arange(1, 10), phq_items, rotation=45)\n",
    "plt.xlabel('PHQ-9 item', fontsize=14)\n",
    "plt.ylabel('Regression coefficient', fontsize=14)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.savefig(fname=outdir+'allNegModelCoefs.pdf', format='pdf', dpi=resolution, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
